{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bloom Filter</h3>\n",
    "1. Preparing data from nltk.<br>\n",
    "2.1 Implementing Bloom filter.<br>\n",
    "2.2 Implementing a naive solution.<br>\n",
    "3. Adding time and memory measurements.<br>\n",
    "4. Implementing edit distance which returns 3 most similar words.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bloom_filter2 as bf\n",
    "from nltk.corpus import words, movie_reviews\n",
    "import nltk\n",
    "import time\n",
    "from random import shuffle\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 - Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "all_words = tokenizer.tokenize(movie_reviews.raw())\n",
    "searched_words = nltk.FreqDist(all_words).most_common()[-10:]\n",
    "searched_words = [i[0] for i in searched_words]\n",
    "words_not_included = ['randomword','bleble','logitech','google','w3school','smartphone','glas','lkjklhg']\n",
    "searched_words = searched_words + words_not_included\n",
    "shuffle(all_words)\n",
    "shuffle(searched_words)\n",
    "print(searched_words)\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.1 - Bloom filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bloom_filter(keys, dataset):\n",
    "    global res_bloom\n",
    "    bloom = bf.BloomFilter(max_elements=len(dataset),error_rate=0.1)\n",
    "    for d in dataset:\n",
    "        if d not in bloom:\n",
    "            bloom.add(d)   \n",
    "    t = time.time()\n",
    "    print(bloom)\n",
    "    for k in keys:\n",
    "        if k not in bloom:\n",
    "            print(f'\"{k}\" is definitely not present.')\n",
    "            res_bloom[k] = 0\n",
    "        elif k in bloom:\n",
    "            if k in words_not_included:\n",
    "                print(f'\"{k}\" - False positive')\n",
    "                res_bloom[k] = 0\n",
    "            else:\n",
    "                print(f'\"{k}\" is probably presant.')\n",
    "                res_bloom[k] = 1\n",
    "    t = time.time()-t\n",
    "    print(f'Time used: {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bloom = {}\n",
    "mem_bloom = memory_usage((Bloom_filter,(searched_words,all_words,)),max_usage=True)\n",
    "print(f'Maximum memory usage: {mem_bloom}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.2 - Naive filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_filter(keys,dataset):\n",
    "    global res_naive\n",
    "    t = time.time()\n",
    "    res = {}\n",
    "    for k in keys:\n",
    "        for d in dataset:\n",
    "            if k == d:\n",
    "                res_naive[k] = 1\n",
    "                break\n",
    "            else:\n",
    "                res_naive[k] = 0\n",
    "    for i in res_naive:\n",
    "        if res_naive[i] == 1:\n",
    "            print(f'\"{i}\" is presant.')\n",
    "        elif res_naive[i] == 0:\n",
    "            print(f'\"{i}\" is definitely not present.')\n",
    "    t = time.time()-t\n",
    "    print(f'Time used: {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_naive = {}\n",
    "mem_naive = memory_usage((Naive_filter,(searched_words,all_words,)),max_usage = True)\n",
    "print(f'Maximum memory usage: {mem_naive}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4 - Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Levenshtein(word,dataset):\n",
    "    ans = []\n",
    "    dataset = list(dict.fromkeys(dataset))\n",
    "    for d in dataset:\n",
    "        ans.append(nltk.edit_distance(word,d))\n",
    "    for _ in range(3):\n",
    "        print(f'Closest word: {dataset[ans.index(min(ans))]} - Edit distance: {min(ans)}')\n",
    "        dataset.pop(ans.index(min(ans)))\n",
    "        ans.pop(ans.index(min(ans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res_bloom:\n",
    "    if res_bloom[i] == 0:\n",
    "        print(f'Word: {i}')\n",
    "        Levenshtein(i,all_words)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbbfc2722fa4c55cfe558c23869bf55e6e125f72a9d3170da96cfc6383575f5f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
